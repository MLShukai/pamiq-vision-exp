# PAMIQ-Core é–‹ç™ºã‚¬ã‚¤ãƒ‰ & ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ

> ç§ãŒ pamiq-core ã‚’ä½¿ã£ã¦é–‹ç™ºã™ã‚‹éš›ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## ğŸ“‘ ç›®æ¬¡

01. [åŸºæœ¬ã‚³ãƒ³ã‚»ãƒ—ãƒˆ](#%E5%9F%BA%E6%9C%AC%E3%82%B3%E3%83%B3%E3%82%BB%E3%83%97%E3%83%88)
02. [ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ](#%E3%82%AF%E3%82%A4%E3%83%83%E3%82%AF%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88)
03. [ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ](#%E3%82%B3%E3%82%A2%E3%82%B3%E3%83%B3%E3%83%9D%E3%83%BC%E3%83%8D%E3%83%B3%E3%83%88)
04. [ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼](#%E3%83%87%E3%83%BC%E3%82%BF%E3%83%95%E3%83%AD%E3%83%BC)
05. [PyTorchçµ±åˆ](#pytorch%E7%B5%B1%E5%90%88)
06. [Gymnasiumçµ±åˆ](#gymnasium%E7%B5%B1%E5%90%88)
07. [State Persistence](#state-persistence)
08. [Consoleåˆ¶å¾¡](#console%E5%88%B6%E5%BE%A1)
09. [Wrappers](#wrappers)
10. [ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³](#%E3%82%88%E3%81%8F%E3%81%82%E3%82%8B%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3)
11. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#%E3%83%88%E3%83%A9%E3%83%96%E3%83%AB%E3%82%B7%E3%83%A5%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0)

______________________________________________________________________

## åŸºæœ¬ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

### PAMIQ-Core ã¨ã¯

**pamiq-core** ã¯ã€æ¨è«–ã¨è¨“ç·´ã®éåŒæœŸå®Ÿè¡Œã‚’å¯èƒ½ã«ã™ã‚‹ã€æœ€å°é™ã®æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚

**è¨­è¨ˆå“²å­¦**:

- **ã‚·ãƒ³ãƒ—ãƒ«ã•** - ç›´æ„Ÿçš„ã§ä½¿ã„ã‚„ã™ã„API
- **è»½é‡** - æœ€å°é™ã®ä¾å­˜é–¢ä¿‚ã€æœ€å¤§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **å®Œå…¨ãªã‚¹ãƒ¬ãƒƒãƒ‰æŠ½è±¡åŒ–** - è¤‡é›‘ãªã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†ã¯å†…éƒ¨ã§å‡¦ç†ã€å¤–éƒ¨ã«ã¯ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å…¨ä½“åƒ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Control Thread (main)                 â”‚
â”‚  - Web API Server (pamiq-consoleæ¥ç¶š)           â”‚
â”‚  - State Persistence                            â”‚
â”‚  - System Lifecycle Management                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  Inference  â”‚      â”‚  Training  â”‚
    â”‚   Thread    â”‚â—„â”€â”€â”€â”€â–ºâ”‚   Thread   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚
    Agent-Env            Model Training
    Interaction          & Sync
```

**3ã¤ã®ä¸¦è¡Œã‚¹ãƒ¬ãƒƒãƒ‰**:

1. **Control Thread** - ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®åˆ¶å¾¡
2. **Inference Thread** - Agent-Environment ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
3. **Training Thread** - ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒæœŸ

______________________________________________________________________

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# åŸºæœ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install pamiq-core

# PyTorchçµ±åˆè¾¼ã¿
pip install pamiq-core[torch]

# Gymnasiumçµ±åˆè¾¼ã¿
pip install pamiq-core[gym]

# ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼è¾¼ã¿
pip install pamiq-core[kbctl]
```

### æœ€å°é™ã®å®Ÿè£…ä¾‹

```python
from pamiq_core import Agent, Environment, Interaction, LaunchConfig, launch
from typing import override

# 1. Agent ã‚’å®Ÿè£…
class MyAgent(Agent[ObsType, ActionType]):
    @override
    def step(self, observation: ObsType) -> ActionType:
        # è¦³æ¸¬ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®š
        return action

# 2. Environment ã‚’å®Ÿè£…
class MyEnvironment(Environment[ObsType, ActionType]):
    @override
    def observe(self) -> ObsType:
        # ç’°å¢ƒã‹ã‚‰è¦³æ¸¬ã‚’å–å¾—
        return observation

    @override
    def affect(self, action: ActionType) -> None:
        # è¡Œå‹•ã‚’ç’°å¢ƒã«é©ç”¨
        pass

# 3. ã‚·ã‚¹ãƒ†ãƒ ã‚’èµ·å‹•
launch(
    interaction=Interaction(MyAgent(), MyEnvironment()),
    models={},
    buffers={},
    trainers={},
    config=LaunchConfig(
        states_dir="./states",
        web_api_address=("localhost", 8391),
    )
)
```

______________________________________________________________________

## ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

### 1. Agentï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç’°å¢ƒã‹ã‚‰è¦³æ¸¬ã‚’å—ã‘å–ã‚Šã€è¡Œå‹•ã‚’æ±ºå®šã—ã¾ã™ã€‚

#### åŸºæœ¬å®Ÿè£…

```python
from pamiq_core import Agent
from typing import override

class MyAgent(Agent[ObservationType, ActionType]):
    @override
    def step(self, observation: ObservationType) -> ActionType:
        """è¦³æ¸¬ã‚’å‡¦ç†ã—ã¦è¡Œå‹•ã‚’è¿”ã™"""
        # æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
        return action
```

#### Inference Model ã®ä½¿ç”¨

```python
class MyAgent(Agent[ObservationType, ActionType]):
    @override
    def on_inference_models_attached(self) -> None:
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«ãŒã‚¢ã‚¿ãƒƒãƒã•ã‚ŒãŸæ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        self.policy = self.get_inference_model("policy")
        self.value = self.get_inference_model("value")

    @override
    def step(self, observation: ObservationType) -> ActionType:
        action = self.policy(observation)  # ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–
        return action
```

#### Data Collector ã®ä½¿ç”¨

```python
class MyAgent(Agent[ObservationType, ActionType]):
    @override
    def on_data_collectors_attached(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ãŒã‚¢ã‚¿ãƒƒãƒã•ã‚ŒãŸæ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        self.collector = self.get_data_collector("experience")

    @override
    def step(self, observation: ObservationType) -> ActionType:
        action = self.decide_action(observation)

        # ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        self.collector.collect({
            "observation": observation,
            "action": action,
            "timestamp": time.time()
        })

        return action
```

#### éšå±¤çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆComposite Agentï¼‰

```python
class MainAgent(Agent[..., ...]):
    def __init__(self) -> None:
        # å­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ
        self.navigation = NavigationAgent()
        self.perception = PerceptionAgent()

        # è¦ªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã—ã¦åˆæœŸåŒ–ï¼ˆå­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«è‡ªå‹•çš„ã«ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ãŒä¼æ’­ï¼‰
        super().__init__(agents={
            "navigation": self.navigation,
            "perception": self.perception
        })
```

### 2. Environmentï¼ˆç’°å¢ƒï¼‰

ç’°å¢ƒã¯è¦³æ¸¬ã‚’ç”Ÿæˆã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚

#### åŸºæœ¬å®Ÿè£…

```python
from pamiq_core import Environment
from typing import override

class MyEnvironment(Environment[ObsType, ActionType]):
    @override
    def observe(self) -> ObsType:
        """ç¾åœ¨ã®è¦³æ¸¬ã‚’è¿”ã™"""
        return self.get_current_state()

    @override
    def affect(self, action: ActionType) -> None:
        """è¡Œå‹•ã‚’ç’°å¢ƒã«é©ç”¨"""
        self.apply_action(action)
```

#### Modular Environmentï¼ˆã‚»ãƒ³ã‚µãƒ¼/ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿ãƒ¼åˆ†é›¢ï¼‰

```python
from pamiq_core import ModularEnvironment, Sensor, Actuator
from typing import override

class CameraSensor(Sensor[np.ndarray]):
    @override
    def read(self) -> np.ndarray:
        """ã‚«ãƒ¡ãƒ©ã‹ã‚‰ç”»åƒã‚’å–å¾—"""
        return capture_image()

class MotorActuator(Actuator[dict]):
    @override
    def operate(self, action: dict) -> None:
        """ãƒ¢ãƒ¼ã‚¿ãƒ¼ã‚’åˆ¶å¾¡"""
        control_motors(action)

# çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨
env = ModularEnvironment(CameraSensor(), MotorActuator())
```

#### è¤‡æ•°ã‚»ãƒ³ã‚µãƒ¼/ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿ãƒ¼

```python
from pamiq_core import SensorsDict, ActuatorsDict

sensors = SensorsDict({
    "camera": CameraSensor(),
    "audio": AudioSensor(),
    "imu": IMUSensor()
})

actuators = ActuatorsDict({
    "motor": MotorActuator(),
    "speaker": SpeakerActuator()
})

env = ModularEnvironment(sensors, actuators)
# agent.step() ã¯ dict[str, Any] ã‚’å—ã‘å–ã‚Šã€dict[str, Any] ã‚’è¿”ã™
```

### 3. Interactionï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ï¼‰

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ç’°å¢ƒã®ç›¸äº’ä½œç”¨ã‚’ç®¡ç†ã—ã¾ã™ã€‚

#### åŸºæœ¬çš„ãª Interaction

```python
from pamiq_core import Interaction

interaction = Interaction(agent, environment)
```

#### Fixed Interval Interactionï¼ˆå›ºå®šé–“éš”å®Ÿè¡Œï¼‰

```python
from pamiq_core import FixedIntervalInteraction

# 10Hzï¼ˆ0.1ç§’é–“éš”ï¼‰ã§å®Ÿè¡Œ
interaction = FixedIntervalInteraction.with_sleep_adjustor(
    agent=agent,
    environment=environment,
    interval=0.1  # ç§’å˜ä½
)
```

### 4. Modelï¼ˆãƒ¢ãƒ‡ãƒ«ï¼‰

PAMIQ-Core ã¯æ¨è«–ãƒ¢ãƒ‡ãƒ«ã¨è¨“ç·´ãƒ¢ãƒ‡ãƒ«ã‚’åˆ†é›¢ã—ã¾ã™ã€‚

#### ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…

```python
from pamiq_core import InferenceModel, TrainingModel
from typing import override

# æ¨è«–ãƒ¢ãƒ‡ãƒ«
class MyInferenceModel(InferenceModel):
    def __init__(self, weights: list[float]):
        self.weights = weights

    @override
    def infer(self, features: list[float]) -> float:
        return sum(w * x for w, x in zip(self.weights, features))

# è¨“ç·´ãƒ¢ãƒ‡ãƒ«
class MyTrainingModel(TrainingModel[MyInferenceModel]):
    def __init__(self):
        super().__init__(
            has_inference_model=True,      # æ¨è«–ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹
            inference_thread_only=False    # æ¨è«–ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã¿ã§ä½¿ç”¨ã™ã‚‹ã‹
        )
        self.weights = [0.5, 0.3, -0.2]

    @override
    def _create_inference_model(self) -> MyInferenceModel:
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
        return MyInferenceModel(self.weights.copy())

    @override
    def forward(self, features: list[float]) -> float:
        """è¨“ç·´æ™‚ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹"""
        return sum(w * x for w, x in zip(self.weights, features))

    @override
    def sync_impl(self, inference_model: MyInferenceModel) -> None:
        """è¨“ç·´ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ¨è«–ãƒ¢ãƒ‡ãƒ«ã¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŒæœŸ"""
        inference_model.weights = self.weights.copy()
```

**é‡è¦ãªè¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:

- `has_inference_model=True`: æ¨è«–ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆãƒ»ç®¡ç†ã™ã‚‹
- `has_inference_model=False`: è¨“ç·´ã®ã¿ã«ä½¿ç”¨ï¼ˆæ¨è«–ãƒ¢ãƒ‡ãƒ«ãªã—ï¼‰
- `inference_thread_only=True`: æ¨è«–ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã¿ã§ä½¿ç”¨ï¼ˆè¨“ç·´ã§æ›´æ–°ã—ãªã„ï¼‰
- `inference_thread_only=False`: æ¨è«–ã¨è¨“ç·´ã®ä¸¡æ–¹ã§ä½¿ç”¨

### 5. Trainerï¼ˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼‰

ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

#### åŸºæœ¬å®Ÿè£…

```python
from pamiq_core import Trainer
from typing import override

class MyTrainer(Trainer):
    def __init__(self):
        super().__init__(
            training_condition_data_user="experience",  # è¨“ç·´æ¡ä»¶ã«ä½¿ã†ãƒãƒƒãƒ•ã‚¡
            min_buffer_size=1000,                       # æœ€å°ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º
            min_new_data_count=100                      # æœ€å°æ–°è¦ãƒ‡ãƒ¼ã‚¿æ•°
        )

    @override
    def on_training_models_attached(self) -> None:
        """è¨“ç·´ãƒ¢ãƒ‡ãƒ«ãŒã‚¢ã‚¿ãƒƒãƒã•ã‚ŒãŸæ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        self.model = self.get_training_model("policy")

    @override
    def on_data_users_attached(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¢ã‚¿ãƒƒãƒã•ã‚ŒãŸæ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        self.data = self.get_data_user("experience")

    @override
    def train(self) -> None:
        """è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…"""
        self.data.update()  # ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
        experiences = self.data.get_data()

        # è¨“ç·´å‡¦ç†
        for batch in create_batches(experiences):
            loss = compute_loss(self.model, batch)
            optimize(loss)
```

#### ã‚«ã‚¹ã‚¿ãƒ è¨“ç·´æ¡ä»¶

```python
@override
def is_trainable(self) -> bool:
    """è¨“ç·´ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã‚’åˆ¤å®š"""
    if not super().is_trainable():
        return False

    # ã‚«ã‚¹ã‚¿ãƒ æ¡ä»¶ã‚’è¿½åŠ 
    return self.custom_condition_met()
```

### 6. Dataï¼ˆãƒ‡ãƒ¼ã‚¿ï¼‰

ãƒ‡ãƒ¼ã‚¿ã®åé›†ã€ä¿å­˜ã€ç®¡ç†ã‚’è¡Œã„ã¾ã™ã€‚

#### çµ„ã¿è¾¼ã¿ãƒãƒƒãƒ•ã‚¡

```python
from pamiq_core.data.impls import (
    SequentialBuffer,           # é †æ¬¡ãƒãƒƒãƒ•ã‚¡
    RandomReplacementBuffer     # ãƒ©ãƒ³ãƒ€ãƒ ç½®æ›ãƒãƒƒãƒ•ã‚¡
)

# æœ€å¤§1024ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ ç½®æ›ãƒãƒƒãƒ•ã‚¡
buffer = RandomReplacementBuffer(max_size=1024)

# æœ€å¤§512ã‚µãƒ³ãƒ—ãƒ«ã®é †æ¬¡ãƒãƒƒãƒ•ã‚¡
buffer = SequentialBuffer(max_size=512)
```

#### ã‚«ã‚¹ã‚¿ãƒ ãƒãƒƒãƒ•ã‚¡ã®å®Ÿè£…

```python
from pamiq_core.data import DataBuffer
from typing import override

class MyBuffer[T](DataBuffer[T, list[T]]):
    @override
    def __init__(self, max_size: int) -> None:
        super().__init__(max_queue_size=max_size)
        self._data: list[T] = []
        self._max_size = max_size

    @override
    def add(self, data: T) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ """
        if len(self._data) >= self._max_size:
            self._data.pop(0)
        self._data.append(data)

    @override
    def get_data(self) -> list[T]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return self._data.copy()

    @override
    def __len__(self) -> int:
        return len(self._data)
```

### 7. Launchï¼ˆèµ·å‹•ï¼‰

ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’èµ·å‹•ã—ã¾ã™ã€‚

#### åŸºæœ¬çš„ãªèµ·å‹•

```python
from pamiq_core import launch, LaunchConfig

launch(
    interaction=interaction,
    models={"model_name": training_model},
    buffers={"buffer_name": data_buffer},
    trainers={"trainer_name": trainer},
    config=LaunchConfig(
        states_dir="./states",
        web_api_address=("localhost", 8391)
    )
)
```

#### LaunchConfig ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³

```python
from pamiq_core.state_persistence import PeriodicSaveCondition, LatestStatesKeeper

LaunchConfig(
    # çŠ¶æ…‹ã®ä¿å­˜å…ˆ
    states_dir="./states",

    # æ—¢å­˜ã®çŠ¶æ…‹ã‹ã‚‰å†é–‹
    saved_state_path="./states/checkpoint_001.state",

    # Web APIï¼ˆpamiq-consoleæ¥ç¶šç”¨ï¼‰
    web_api_address=("localhost", 8391),  # Noneã§ç„¡åŠ¹åŒ–

    # å®Ÿè¡Œæ™‚é–“åˆ¶é™ï¼ˆç§’ï¼‰
    max_uptime=3600.0,  # 1æ™‚é–“ã§çµ‚äº†

    # æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆé«˜é€ŸåŒ–ãƒ»ä½é€ŸåŒ–ï¼‰
    time_scale=2.0,  # 2å€é€Ÿ

    # çŠ¶æ…‹ä¿å­˜ã®æ¡ä»¶
    save_state_condition=PeriodicSaveCondition(600.0),  # 10åˆ†æ¯

    # çŠ¶æ…‹ã®ä¿æŒæ•°
    states_keeper=LatestStatesKeeper(
        states_dir="./states",
        max_keep=5  # æœ€æ–°5ã¤ã®ã¿ä¿æŒ
    )
)
```

______________________________________________________________________

## ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

### Agent â†’ Buffer â†’ Trainer ã®å®Œå…¨ãªæµã‚Œ

```python
# 1. ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆAgent - Inference Threadï¼‰
class MyAgent(Agent[obs, act]):
    def on_data_collectors_attached(self) -> None:
        self.collector = self.get_data_collector("experience")

    def step(self, obs: obs) -> act:
        action = self.decide(obs)
        self.collector.collect({"obs": obs, "act": action})  # åé›†
        return action

# 2. ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡
buffer = RandomReplacementBuffer(max_size=10000)

# 3. ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼ˆTrainer - Training Threadï¼‰
class MyTrainer(Trainer):
    def on_data_users_attached(self) -> None:
        self.data_user = self.get_data_user("experience")

    def train(self) -> None:
        self.data_user.update()  # Collector â†’ Buffer ã¸è»¢é€
        data = self.data_user.get_data()  # Buffer ã‹ã‚‰å–å¾—

        # è¨“ç·´å‡¦ç†
        for batch in create_batches(data):
            train_step(batch)

# 4. ã‚·ã‚¹ãƒ†ãƒ ã«ç™»éŒ²
launch(
    interaction=Interaction(MyAgent(), env),
    models={"model": training_model},
    buffers={"experience": buffer},  # ãƒãƒƒãƒ•ã‚¡ã‚’ç™»éŒ²
    trainers={"trainer": MyTrainer()},
    config=config
)
```

______________________________________________________________________

## PyTorchçµ±åˆ

### TorchTrainingModel ã®ä½¿ç”¨

```python
import torch
import torch.nn as nn
from pamiq_core.torch import TorchTrainingModel

# PyTorchãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©
class PolicyNet(nn.Module):
    def __init__(self, input_dim: int, output_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, output_dim)
        )

    def forward(self, x):
        return self.net(x)

    def infer(self, x):
        """æ¨è«–æ™‚ã®å‡¦ç†ï¼ˆç•°ãªã‚‹å ´åˆã®ã¿å®šç¾©ï¼‰"""
        with torch.no_grad():
            return self.forward(x)

# PAMIQ-Core ã§ä½¿ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = TorchTrainingModel(
    PolicyNet(input_dim=10, output_dim=4),
    inference_procedure=PolicyNet.infer,  # æ¨è«–æ™‚ã®å‡¦ç†ã‚’æŒ‡å®š
    device=device
)
```

### TorchTrainer ã®å®Ÿè£…

```python
from pamiq_core.torch import TorchTrainer, OptimizersSetup
import torch.optim as optim

class MyTorchTrainer(TorchTrainer):
    def __init__(self):
        super().__init__(
            training_condition_data_user="experience",
            min_buffer_size=1000,
            min_new_data_count=100
        )

    @override
    def on_training_models_attached(self) -> None:
        super().on_training_models_attached()
        # TorchTrainingModelã‚’å–å¾—ï¼ˆå‹ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
        self.policy = self.get_torch_training_model("policy", PolicyNet)

    @override
    def create_optimizers(self) -> OptimizersSetup:
        """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆ"""
        self.optimizer = optim.Adam(self.policy.model.parameters(), lr=1e-3)
        return {"optimizer": self.optimizer}

    @override
    def train(self) -> None:
        """è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯"""
        self.data_user.update()
        data = self.data_user.get_data()

        # ãƒãƒƒãƒä½œæˆ
        dataloader = create_dataloader(data, batch_size=32)

        for batch in dataloader:
            # Forward
            output = self.policy(batch["input"])
            loss = compute_loss(output, batch["target"])

            # Backward
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
```

### å®Œå…¨ãªPyTorchã®ä¾‹ï¼ˆVAEï¼‰

```python
# ãƒ¢ãƒ‡ãƒ«å®šç¾©
class Encoder(nn.Module):
    def __init__(self, feature_size: int):
        super().__init__()
        # ... ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å®šç¾© ...

    def forward(self, x):
        # è¨“ç·´æ™‚ã®å‡¦ç†
        return distribution

    def infer(self, x):
        # æ¨è«–æ™‚ã®å‡¦ç†ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãªã—ï¼‰
        with torch.no_grad():
            return self.forward(x).mean

class Decoder(nn.Module):
    def __init__(self, feature_size: int):
        super().__init__()
        # ... ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®å®šç¾© ...

# ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

models = {
    "encoder": TorchTrainingModel(
        Encoder(feature_size=64),
        inference_procedure=Encoder.infer,
        device=device
    ),
    "decoder": TorchTrainingModel(
        Decoder(feature_size=64),
        has_inference_model=False,  # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã¯è¨“ç·´ã®ã¿
        device=device
    )
}

launch(
    interaction=FixedIntervalInteraction.with_sleep_adjustor(
        agent=EncodingAgent(),
        environment=EncodingCheckEnv(feature_size=64),
        interval=0.1
    ),
    models=models,
    buffers={"observation": RandomReplacementBuffer(max_size=1024)},
    trainers={"vae": VAETrainer(max_epochs=3, batch_size=32)},
    config=LaunchConfig(states_dir="./states")
)
```

______________________________________________________________________

## Gymnasiumçµ±åˆ

Gymnasiumç’°å¢ƒã‚’PAMIQ-Coreã§ä½¿ç”¨ã§ãã¾ã™ã€‚

### GymEnvironment ã®ä½¿ç”¨

```python
from pamiq_core.gym import GymEnvironment

# ç’°å¢ƒIDã‹ã‚‰ä½œæˆ
env = GymEnvironment("CartPole-v1")

# æ—¢å­˜ã®Gymnasiumç’°å¢ƒã‚’ä½¿ç”¨
import gymnasium as gym
gym_env = gym.make("CartPole-v1", render_mode="human")
env = GymEnvironment(gym_env)
```

### GymAgent ã®å®Ÿè£…

```python
from pamiq_core.gym import GymAgent
import numpy as np
from typing import override

class CartPoleAgent(GymAgent[np.ndarray, int]):
    @override
    def on_reset(self, obs, info):
        """ç’°å¢ƒãƒªã‚»ãƒƒãƒˆæ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        return 0  # åˆæœŸè¡Œå‹•

    @override
    def on_step(self, obs, reward, terminated, truncated, info):
        """å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‘¼ã°ã‚Œã‚‹"""
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒªã‚·ãƒ¼: ãƒãƒ¼ãƒ«ãŒå³ã«å‚¾ã„ãŸã‚‰å³ã«ç§»å‹•
        return 1 if obs[2] > 0 else 0
```

### æ‰‹å‹•ãƒªã‚»ãƒƒãƒˆ

```python
class EarlyStoppingAgent(GymAgent[np.ndarray, int]):
    @override
    def on_step(self, obs, reward, terminated, truncated, info):
        # æ¡ä»¶ã‚’æº€ãŸã—ãŸã‚‰ãƒªã‚»ãƒƒãƒˆã‚’è¦æ±‚
        if abs(obs[2]) > 0.5:
            self.need_reset = True
        return 1 if obs[2] > 0 else 0
```

### å®Œå…¨ãªGymnasiumã®ä¾‹

```python
from pamiq_core import launch, LaunchConfig, Interaction
from pamiq_core.gym import GymEnvironment, GymAgent

class MyGymAgent(GymAgent[np.ndarray, int]):
    def on_inference_models_attached(self) -> None:
        self.policy = self.get_inference_model("policy")

    def on_data_collectors_attached(self) -> None:
        self.collector = self.get_data_collector("experience")

    def on_reset(self, obs, info):
        return self.policy(obs)

    def on_step(self, obs, reward, terminated, truncated, info):
        action = self.policy(obs)

        # çµŒé¨“ã‚’åé›†
        self.collector.collect({
            "obs": obs,
            "action": action,
            "reward": reward,
            "done": terminated or truncated
        })

        return action

# ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•
launch(
    interaction=Interaction(
        MyGymAgent(),
        GymEnvironment("CartPole-v1")
    ),
    models={"policy": policy_model},
    buffers={"experience": buffer},
    trainers={"trainer": trainer},
    config=LaunchConfig(states_dir="./states")
)
```

______________________________________________________________________

## State Persistence

### ã‚«ã‚¹ã‚¿ãƒ çŠ¶æ…‹ã®ä¿å­˜ã¨å¾©å…ƒ

```python
from pathlib import Path
from pamiq_core import Agent
from typing import override

class MyCustomAgent(Agent[float, int]):
    def __init__(self):
        super().__init__()
        self.episode_count = 0
        self.total_reward = 0.0
        self.learning_rate = 0.01

    @override
    def save_state(self, path: Path) -> None:
        """ã‚«ã‚¹ã‚¿ãƒ çŠ¶æ…‹ã‚’ä¿å­˜"""
        super().save_state(path)  # è¦ªã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å¿…ãšå‘¼ã¶

        path.mkdir(exist_ok=True)
        with open(path / "stats.txt", "w") as f:
            f.write(f"episode_count: {self.episode_count}\n")
            f.write(f"total_reward: {self.total_reward}\n")
            f.write(f"learning_rate: {self.learning_rate}\n")

    @override
    def load_state(self, path: Path) -> None:
        """ã‚«ã‚¹ã‚¿ãƒ çŠ¶æ…‹ã‚’å¾©å…ƒ"""
        super().load_state(path)  # è¦ªã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å¿…ãšå‘¼ã¶

        try:
            with open(path / "stats.txt", "r") as f:
                for line in f:
                    key, value = line.strip().split(": ")
                    if key == "episode_count":
                        self.episode_count = int(value)
                    elif key == "total_reward":
                        self.total_reward = float(value)
                    elif key == "learning_rate":
                        self.learning_rate = float(value)
        except FileNotFoundError:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
            pass
```

### çŠ¶æ…‹ã®ä¿å­˜æ¡ä»¶

```python
from pamiq_core.state_persistence import PeriodicSaveCondition, LatestStatesKeeper

# å®šæœŸçš„ã«ä¿å­˜
config = LaunchConfig(
    states_dir="./states",
    save_state_condition=PeriodicSaveCondition(300.0),  # 5åˆ†æ¯
    states_keeper=LatestStatesKeeper(
        states_dir="./states",
        max_keep=10  # æœ€æ–°10å€‹ã‚’ä¿æŒ
    )
)
```

### çŠ¶æ…‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
[timestamp].state/
â”œâ”€â”€ interaction/
â”‚   â”œâ”€â”€ agent/          # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹
â”‚   â””â”€â”€ environment/    # ç’°å¢ƒã®çŠ¶æ…‹
â”œâ”€â”€ models/             # ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹
â”œâ”€â”€ data/               # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡ã®çŠ¶æ…‹
â”œâ”€â”€ trainers/           # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®çŠ¶æ…‹
â””â”€â”€ time.pkl            # æ™‚é–“åˆ¶å¾¡ã®çŠ¶æ…‹
```

______________________________________________________________________

## Consoleåˆ¶å¾¡

### pamiq-console ã®ä½¿ç”¨

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«æ¥ç¶š
pamiq-console --host localhost --port 8391

# ãƒªãƒ¢ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ã«æ¥ç¶š
pamiq-console --host 192.168.1.100 --port 8391
```

### åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰

```
pamiq-console (active) > help

Available commands:
  h, help     - Show this help message
  p, pause    - Pause the system
  r, resume   - Resume the system
  save        - Save current state
  shutdown    - Shutdown the system (requires confirmation)
  q, quit     - Exit console (system continues running)
```

### Web API ã®ä½¿ç”¨

```bash
# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—
curl http://localhost:8391/api/status

# ä¸€æ™‚åœæ­¢
curl -X POST http://localhost:8391/api/pause

# å†é–‹
curl -X POST http://localhost:8391/api/resume

# çŠ¶æ…‹ä¿å­˜
curl -X POST http://localhost:8391/api/save-state

# ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
curl -X POST http://localhost:8391/api/shutdown
```

### ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install pamiq-core[kbctl]

# Linux ã®å ´åˆã€ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt-get install libevdev-dev build-essential

# èµ·å‹•
pamiq-kbctl --host localhost --port 8391
```

**ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ**:

- Windows/Linux: `Alt+Shift+P` (ä¸€æ™‚åœæ­¢), `Alt+Shift+R` (å†é–‹)
- macOS: `Option+Shift+P` (ä¸€æ™‚åœæ­¢), `Option+Shift+R` (å†é–‹)

______________________________________________________________________

## Wrappers

ç’°å¢ƒã‚„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‹•ä½œã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½œæˆã§ãã¾ã™ã€‚

### EnvironmentWrapper ã®ä½¿ç”¨

```python
from pamiq_core import EnvironmentWrapper
from typing import override

class LoggingEnvironmentWrapper(EnvironmentWrapper[ObsType, ActionType]):
    """è¦³æ¸¬ã¨è¡Œå‹•ã‚’ãƒ­ã‚®ãƒ³ã‚°ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼"""

    @override
    def observe(self) -> ObsType:
        obs = super().observe()
        print(f"Observation: {obs}")
        return obs

    @override
    def affect(self, action: ActionType) -> None:
        print(f"Action: {action}")
        super().affect(action)

# ä½¿ç”¨
wrapped_env = LoggingEnvironmentWrapper(original_env)
```

### LambdaWrapper ã®ä½¿ç”¨

```python
from pamiq_core import LambdaWrapper

# è¦³æ¸¬ã‚’æ­£è¦åŒ–ã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼
normalize_obs = LambdaWrapper(
    lambda obs: (obs - obs.mean()) / (obs.std() + 1e-8)
)

wrapped_env = normalize_obs.wrap_environment(original_env)
```

### SensorWrapper ã¨ ActuatorWrapper

```python
from pamiq_core import SensorWrapper, ActuatorWrapper

# ã‚»ãƒ³ã‚µãƒ¼ã«ãƒã‚¤ã‚ºã‚’è¿½åŠ 
class NoisySensorWrapper(SensorWrapper[np.ndarray]):
    @override
    def read(self) -> np.ndarray:
        data = super().read()
        noise = np.random.randn(*data.shape) * 0.1
        return data + noise

# ã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿ãƒ¼ã«é…å»¶ã‚’è¿½åŠ 
class DelayedActuatorWrapper(ActuatorWrapper[ActionType]):
    def __init__(self, actuator, delay: float):
        super().__init__(actuator)
        self.delay = delay

    @override
    def operate(self, action: ActionType) -> None:
        time.sleep(self.delay)
        super().operate(action)
```

______________________________________________________________________

## ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³

### ãƒ‘ã‚¿ãƒ¼ãƒ³1: å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

```python
class RLAgent(Agent[np.ndarray, int]):
    def on_inference_models_attached(self) -> None:
        self.policy = self.get_inference_model("policy")
        self.value = self.get_inference_model("value")

    def on_data_collectors_attached(self) -> None:
        self.experience = self.get_data_collector("experience")

    def step(self, observation: np.ndarray) -> int:
        # ãƒãƒªã‚·ãƒ¼ã§è¡Œå‹•ã‚’é¸æŠ
        action_probs = self.policy(observation)
        action = np.random.choice(len(action_probs), p=action_probs)

        # ä¾¡å€¤æ¨å®š
        value = self.value(observation)

        # çµŒé¨“ã‚’ä¿å­˜
        self.experience.collect({
            "state": observation,
            "action": action,
            "value": value
        })

        return action
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³2: è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’

```python
class SelfSupervisedAgent(Agent[torch.Tensor, torch.Tensor]):
    def on_inference_models_attached(self) -> None:
        self.encoder = self.get_inference_model("encoder")

    def on_data_collectors_attached(self) -> None:
        self.data = self.get_data_collector("observations")

    def step(self, observation: torch.Tensor) -> torch.Tensor:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        encoding = self.encoder(observation)

        # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆè¨“ç·´ç”¨ï¼‰
        self.data.collect({"obs": observation.cpu()})

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿”ã™ï¼ˆç’°å¢ƒã§æ¤œè¨¼ãªã©ï¼‰
        return encoding
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³3: çŠ¶æ…‹ã®ä¿å­˜ã¨å¾©å…ƒ

```python
from pathlib import Path
from pamiq_core.state_persistence import PeriodicSaveCondition, LatestStatesKeeper

# åˆå›å®Ÿè¡Œ
launch(
    interaction=interaction,
    models=models,
    buffers=buffers,
    trainers=trainers,
    config=LaunchConfig(
        states_dir="./states",
        save_state_condition=PeriodicSaveCondition(600.0),  # 10åˆ†æ¯ä¿å­˜
        states_keeper=LatestStatesKeeper(
            states_dir="./states",
            max_keep=5
        )
    )
)

# å†é–‹
latest = sorted(Path("./states").glob("*.state"))[-1]
launch(
    interaction=interaction,
    models=models,
    buffers=buffers,
    trainers=trainers,
    config=LaunchConfig(
        states_dir="./states",
        saved_state_path=latest  # æœ€æ–°ã®çŠ¶æ…‹ã‹ã‚‰å†é–‹
    )
)
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³4: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’

```python
from pamiq_core import ModularEnvironment, SensorsDict, ActuatorsDict

# ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚»ãƒ³ã‚µãƒ¼
sensors = SensorsDict({
    "camera": CameraSensor(),
    "lidar": LidarSensor(),
    "audio": AudioSensor()
})

# ãƒãƒ«ãƒã‚¢ã‚¯ãƒãƒ¥ã‚¨ãƒ¼ã‚¿ãƒ¼
actuators = ActuatorsDict({
    "motor": MotorActuator(),
    "arm": ArmActuator(),
    "speaker": SpeakerActuator()
})

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
class MultimodalAgent(Agent[dict, dict]):
    def on_inference_models_attached(self) -> None:
        self.vision_encoder = self.get_inference_model("vision")
        self.audio_encoder = self.get_inference_model("audio")
        self.policy = self.get_inference_model("policy")

    def step(self, observation: dict) -> dict:
        # å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        vision_feat = self.vision_encoder(observation["camera"])
        audio_feat = self.audio_encoder(observation["audio"])

        # çµ±åˆã—ã¦è¡Œå‹•ã‚’æ±ºå®š
        combined = torch.cat([vision_feat, audio_feat], dim=-1)
        actions = self.policy(combined)

        return {
            "motor": actions[:3],
            "arm": actions[3:6],
            "speaker": actions[6:]
        }

# èµ·å‹•
launch(
    interaction=Interaction(
        MultimodalAgent(),
        ModularEnvironment(sensors, actuators)
    ),
    models=models,
    buffers=buffers,
    trainers=trainers,
    config=config
)
```

### ãƒ‘ã‚¿ãƒ¼ãƒ³5: ãƒªãƒ¢ãƒ¼ãƒˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«åˆ¶å¾¡

```bash
# ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•æ™‚ã« web_api_address ã‚’è¨­å®š
# config=LaunchConfig(web_api_address=("localhost", 8391))

# åˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰æ¥ç¶š
pamiq-console --host localhost --port 8391

# ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
pamiq-console (active) > pause     # ä¸€æ™‚åœæ­¢
pamiq-console (paused) > resume    # å†é–‹
pamiq-console (active) > save      # çŠ¶æ…‹ä¿å­˜
pamiq-console (active) > shutdown  # ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
```

______________________________________________________________________

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Q: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ›´æ–°ã•ã‚Œãªã„

**A**: `sync_impl()` ãŒæ­£ã—ãå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
@override
def sync_impl(self, inference_model: MyInferenceModel) -> None:
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºå®Ÿã«ã‚³ãƒ”ãƒ¼
    inference_model.weights = self.weights.copy()  # âœ“
    # inference_model.weights = self.weights  # âœ—ï¼ˆå‚ç…§ã‚³ãƒ”ãƒ¼ï¼‰
```

### Q: ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œãªã„

**A**: `DataUser.update()` ã‚’å‘¼ã³å‡ºã—ã¦ã„ã¾ã™ã‹ï¼Ÿ

```python
def train(self) -> None:
    self.data_user.update()  # ã“ã‚Œã‚’å¿˜ã‚Œãšã«ï¼
    data = self.data_user.get_data()
```

### Q: è¨“ç·´ãŒå®Ÿè¡Œã•ã‚Œãªã„

**A**: è¨“ç·´æ¡ä»¶ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
# min_buffer_size ã¨ min_new_data_count ã‚’ç¢ºèª
trainer = MyTrainer(
    training_condition_data_user="experience",
    min_buffer_size=1000,      # ãƒãƒƒãƒ•ã‚¡ã«1000ã‚µãƒ³ãƒ—ãƒ«å¿…è¦
    min_new_data_count=100     # 100å€‹ã®æ–°è¦ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
)
```

### Q: GPU ãŒä½¿ç”¨ã•ã‚Œãªã„ï¼ˆPyTorchï¼‰

**A**: ãƒ‡ãƒã‚¤ã‚¹ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = TorchTrainingModel(net, device=device)
```

### Q: æ™‚é–“åˆ¶å¾¡ãŒåŠ¹ã‹ãªã„

**A**: `FixedIntervalInteraction` ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ

```python
# âœ“ å›ºå®šé–“éš”ã§å®Ÿè¡Œ
interaction = FixedIntervalInteraction.with_sleep_adjustor(
    agent, environment, interval=0.1
)

# âœ— é€šå¸¸ã®Interactionã¯é–“éš”åˆ¶å¾¡ãªã—
interaction = Interaction(agent, environment)
```

### Q: pamiq-console ãŒæ¥ç¶šã§ããªã„

**A**: Web API ãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
# âœ“ Web APIã‚’æœ‰åŠ¹åŒ–
config = LaunchConfig(web_api_address=("localhost", 8391))

# âœ— Web APIãŒç„¡åŠ¹
config = LaunchConfig(web_api_address=None)
```

### Q: çŠ¶æ…‹ãŒä¿å­˜ã•ã‚Œãªã„

**A**: `save_state_condition` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```python
from pamiq_core.state_persistence import PeriodicSaveCondition

config = LaunchConfig(
    states_dir="./states",
    save_state_condition=PeriodicSaveCondition(600.0)  # 10åˆ†æ¯
)
```

______________________________________________________________________

## å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

### å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [PAMIQ-Core Documentation](https://mlshukai.github.io/pamiq-core/)
- [GitHub - PAMIQ-Core](https://github.com/MLShukai/pamiq-core)

### ã‚µãƒ³ãƒ—ãƒ«

- [minimum.py](pamiq-core/samples/minimum.py) - æœ€å°é™ã®å®Ÿè£…
- [vae-torch/](pamiq-core/samples/vae-torch/) - VAEã®è¨“ç·´ä¾‹

### é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

- [pamiq-recorder](https://github.com/MLShukai/pamiq-recorder) - è¨˜éŒ²ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- [pamiq-io](https://github.com/MLShukai/pamiq-io) - I/Oãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- [pamiq-vrchat](https://github.com/MLShukai/pamiq-vrchat) - VRChatçµ±åˆ

______________________________________________________________________

## ãƒãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ - ã‚ˆãä½¿ã†ã‚³ãƒãƒ³ãƒ‰

### ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

```python
# ã‚³ã‚¢
from pamiq_core import (
    Agent, Environment, Interaction,
    FixedIntervalInteraction, ModularEnvironment,
    Sensor, Actuator, SensorsDict, ActuatorsDict,
    InferenceModel, TrainingModel, Trainer,
    DataBuffer, DataCollector, DataUser,
    launch, LaunchConfig
)

# PyTorch
from pamiq_core.torch import TorchTrainingModel, TorchTrainer, OptimizersSetup

# ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡
from pamiq_core.data.impls import (
    SequentialBuffer, RandomReplacementBuffer
)

# Gymnasium
from pamiq_core.gym import GymEnvironment, GymAgent

# State Persistence
from pamiq_core.state_persistence import (
    PeriodicSaveCondition, LatestStatesKeeper
)

# Wrappers
from pamiq_core import (
    EnvironmentWrapper, LambdaWrapper,
    SensorWrapper, ActuatorWrapper
)
```

### åŸºæœ¬çš„ãªãƒ•ãƒ­ãƒ¼

```python
# 1. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆä½œæˆ
agent = MyAgent()
environment = MyEnvironment()
interaction = Interaction(agent, environment)

# 2. ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒƒãƒ•ã‚¡ãƒ»ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
models = {"model_name": TorchTrainingModel(net, device=device)}
buffers = {"buffer_name": RandomReplacementBuffer(max_size=10000)}
trainers = {"trainer_name": MyTrainer()}

# 3. èµ·å‹•
launch(
    interaction,
    models,
    buffers,
    trainers,
    LaunchConfig(
        states_dir="./states",
        web_api_address=("localhost", 8391)
    )
)
```

### ã‚ˆãä½¿ã†ã‚³ãƒãƒ³ãƒ‰

```bash
# pamiq-console
pamiq-console --host localhost --port 8391

# ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼
pamiq-kbctl --host localhost --port 8391

# APIï¼ˆcurlï¼‰
curl http://localhost:8391/api/status
curl -X POST http://localhost:8391/api/pause
curl -X POST http://localhost:8391/api/resume
curl -X POST http://localhost:8391/api/save-state
```

______________________________________________________________________

**æœ€çµ‚æ›´æ–°**: 2026-01-11
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: pamiq-core æº–æ‹ 
